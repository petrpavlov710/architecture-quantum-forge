## Задание 1. Исследование моделей и инфраструктуры

#### 1. Сравнение LLM-моделей

| Параметр                               | Локальные HugginFace                                                 | Облачные OpenAI/YandexGPT       |
| -------------------------------------- | -------------------------------------------------------------------- | ------------------------------- |
| **Качество ответов**                   | В зависимости от модели и дообучения, например Llama3 (8B/70B/405B ) | Высокий уровень                 |
| **Скорость работы**                    | Зависит от машины, на которой развернута модель                      | Высокая и предсказуемая         |
| **Стоимость владения и использования** | Высокая (Закупка железа/облачные решения + обслуживание и поддержка) | Зависит от объема использования |
| **Удобство и простота развёртывания**  | Требует настройки окружения, оптимизации CPU/GPU, Docker, Kubernetes | Быстро интегрируется через API  |

##### Выводы и рекомендации

- **Для PoC**: использовать облачные решения (инфраструктуру или OpenAPI), так как это быстро, просто, без инфраструктуры
- **Для продакшена**: рассмотреть локальные решения для снижения затрат при высоких нагрузках, но с учётом затрат на железо и поддержку.
- **Для безопасности**: локальные модели могут быть предпочтительнее, так как данные не покидают инфраструктуру.

---

#### 2. Сравнение моделей эмбеддингов

| Параметр                               | Локальные Sentence-Transformers    | Облачные OpenAI Embeddings            |
| -------------------------------------- | ---------------------------------- | ------------------------------------- |
| **Скорость создания индекса**          | Быстро при наличии GPU             | Быстро                                |
| **Качество поиска**                    | Хорошее, можно дообучать под домен | Высокое                               |
| **Стоимость владения и использования** | ФОТ+инфра; без платы за вызов API  | Тарифицируется по факту использования |

##### Выводы и рекомендации

- Для POC использовать OpenAI Embeddings (быстро, без развертывания своей инфраструктуры).
- Sentence-Transformers при необходипости повышеной безопасности
- Sentence-Transformers при небольшом количестве запросов

---

#### 3. Сравнение векторных баз: ChromaDB vs FAISS

| Параметр                                | FAISS                                                       | ChromaDB                                                    |
| --------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------- |
| **Скорость поиска и индексации**        | Очень высокая (C++ ядро); миллисекунды даже на млн векторов | Хорошая; Python + C; несколько десятков мс на 100k векторов |
| **Сложность внедрения и поддержки**     | Встраиваемая библиотека; Легко встроить в Docker            | Запускается как отдельный сервис                            |
| **Удобство в работе**                   | Базовые функции                                             | Расширенные: метаданные, локализация; UI для управления     |
| **Стоимость владения (инфраструктура)** | Почти нулевая                                               | Почти нулевая                                               |

##### Выводы и рекомендации

- **FAISS** для старта: простота, производительность, отсутствие обслуживания сервиса.
- **ChromaDB** для расширенной функциональности: работа с метаданными и масштабирование.

---

#### 4. Рекомендуемая конфигурация сервера

| **Конфигурация**    | **Минимум для продакшн**                          |
| ------------------- | ------------------------------------------------- |
| CPU                 | 4–8 ядер (например, AMD Ryzen или Intel Xeon)     |
| RAM                 | 32-64 GB                                          |
| GPU (если локально) | NVIDIA A100 / RTX 3090 / T4 (для LLM и embedding) |
| Хранилище           | SSD 256–512 GB                                    |
| ОС                  | Ubuntu 22.04+                                     |

| Вариант    | Описание                                                                    | Преимущества                                   | Недостатки                                     |
| ---------- | --------------------------------------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- |
| Облачный   | Cloud OpenAI GPT-4 + OpenAI Embeddings + FAISS / Chroma в Docker            | Быстрое развёртывание, высокое качество        | Высокая стоимость при росте запросов           |
| Гибридный  | Hybrid OpenAI Embeddings + локальная модель (например, LLaMA 3 8B) + Chroma | Баланс между качеством и контролем над данными | Средняя сложность, зависит от GPU              |
| On-premise | Всё локально: Mistral + Sentence-Transformers + FAISS/Chroma                | Полный контроль и безопасность                 | Сложное и дорогое развёртывание, ниже качество |

---

#### Итоговые рекомендации

1. **LLM:** Mistral 7B или Llama3 8B для локального использования, для сокращения затрат и обеспечения безопасности.
2. **Эмбеддинги:** OpenAI Embeddings — простой API, надёжное качество, малые накладные расходы на инфраструктуру.
   2.1 **Альтернатива:** all-MiniLM-L6-v2 - для PoC с целью снижения затрат.
3. **Векторная база:** ChromaDB — REST-интерфейс, GUI для администрирования, быстрое прототипирование.
4. **Инфраструктура:** Вариант B — 8 vCPU, 32 GB, NVIDIA T4; обеспечивает достаточную производительность при умеренных затратах.

**Аргументы:**

- Облачные сервисы позволяют быстрее выйти на пилот.
- ChromaDB снизит порог вхождения для команды без глубокого Python-экспертиза.
- Конфигурация B даёт запас мощностей для роста без значительного удорожания.
